{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nguyen Results\n",
    "\n",
    "This notebook is used to compare the results in RÃ¼disser et al. 2025 to Nguyen et al. 2025\n",
    "\n",
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().resolve().parent.parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import yaml\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "# hide all logs except for errors\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"ERROR\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up matplotlib style\n",
    "sns.set_context(\"talk\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "from scripts.data.visualise.insitu_geosphere import (\n",
    "    prop\n",
    ")\n",
    "\n",
    "geo_cornflowerblue = \"dodgerblue\"\n",
    "geo_lime = \"gold\"\n",
    "geo_magenta = \"firebrick\"\n",
    "\n",
    "from src.arcane2.data.data_utils.event import (\n",
    "    EventCatalog\n",
    ")\n",
    "\n",
    "from src.arcane2.data.realtime.realtime_insitu_dataset import RealtimeInsituDataset\n",
    "from src.arcane2.data.catalogs.nguyen_dataset import Nguyen_Dataset\n",
    "from src.arcane2.data.abstract.multi_signal_dataset import MultiSignalDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load the config file used during training\n",
    "\n",
    "config = yaml.safe_load(\n",
    "    open(project_root / \"config/base_dataset/curated_realtime_dataset_lowres.yaml\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "### Loading results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we prepare the filepaths for the results that were generated during inference\n",
    "\n",
    "result_paths = []\n",
    "\n",
    "run_names = [\"train_arcane_rtsw_new_bounds_new_drops\"]\n",
    "for run_name in run_names:\n",
    "    cache_path = project_root / f\"cache/{run_name}\"\n",
    "\n",
    "    path = (\n",
    "        cache_path\n",
    "        / f\"all_results_curated_realtime_dataset_lowres_tminus_all.pkl\"\n",
    "    )\n",
    "    if path.exists():\n",
    "        result_paths.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load the results and concatenate them into a single dataframe\n",
    "\n",
    "for i, path in enumerate(tqdm(result_paths)):\n",
    "    if i == 0:\n",
    "        all_results = pd.read_pickle(path)\n",
    "    else:\n",
    "        loaded = pd.read_pickle(path)\n",
    "        all_results = pd.concat([all_results, loaded], axis=0).sort_index()\n",
    "        all_results = all_results.combine_first(loaded)\n",
    "        all_results = all_results.groupby(all_results.index).first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we drop the missing values\n",
    "\n",
    "all_results = all_results.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the percentage of missing values in all_results\n",
    "expected_range = pd.date_range(\n",
    "    start=all_results.index.min(),\n",
    "    end=all_results.index.max(),\n",
    "    freq=\"30min\",\n",
    ")\n",
    "missing = expected_range.difference(all_results.index)\n",
    "missing_percentage = len(missing) / len(expected_range) * 100\n",
    "print(\n",
    "    f\"Missing values in all_results: {len(missing)} ({missing_percentage:.2f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the model trained correctly, the minimum value should be close to 0\n",
    "print(f\"Minimum value: {all_results[\"predicted_value_train_arcane_rtsw_new_bounds_new_drops_0_tminus1\"].min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Catalog Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the catalog dataset from the Nguyen Catalogs\n",
    "\n",
    "catalog_paths = [\n",
    "    project_root / \"data/dataverse_files/ICME_catalog_OMNI.csv\",\n",
    "    project_root / \"data/dataverse_files/Sheath_catalog_OMNI.csv\",\n",
    "]\n",
    "event_types = config[\"dataset\"][\"single_signal_datasets\"][0].get(\"event_types\", \"ICME\")\n",
    "filters = config[\"dataset\"][\"single_signal_datasets\"][0].get(\"filters\", None)\n",
    "cap = config[\"dataset\"][\"single_signal_datasets\"][0].get(\"cap\", None)\n",
    "resample_freq = config[\"dataset\"][\"single_signal_datasets\"][0].get(\n",
    "    \"resample_freq\", \"10min\"\n",
    ")\n",
    "\n",
    "catalog_dataset = Nguyen_Dataset(\n",
    "    folder_paths=catalog_paths,\n",
    "    resample_freq=resample_freq,\n",
    "    event_types=event_types,\n",
    "    filters=filters,\n",
    "    cap=cap,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating In Situ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the insitu dataset from the NOAA archive without interpolating the data\n",
    "\n",
    "folder_path = project_root / \"data/noaa_archive_gsm.p\"\n",
    "components = config[\"dataset\"][\"single_signal_datasets\"][1].get(\"components\")\n",
    "resample = config[\"dataset\"][\"single_signal_datasets\"][1].get(\"resample\")\n",
    "resample_method = config[\"dataset\"][\"single_signal_datasets\"][1].get(\"resample_method\")\n",
    "resample_freq = config[\"dataset\"][\"single_signal_datasets\"][1].get(\"resample_freq\")\n",
    "padding = config[\"dataset\"][\"single_signal_datasets\"][1].get(\"padding\")\n",
    "lin_interpol = config['dataset']['single_signal_datasets'][1].get('lin_interpol')\n",
    "scaling = config[\"dataset\"][\"single_signal_datasets\"][1].get(\"scaling\", \"None\")\n",
    "\n",
    "insitu_dataset = RealtimeInsituDataset(\n",
    "    folder_path=folder_path,\n",
    "    components=components,\n",
    "    resample=resample,\n",
    "    resample_freq=resample_freq,\n",
    "    resample_method=resample_method,\n",
    "    padding=padding,\n",
    "    lin_interpol=lin_interpol,\n",
    "    scaling=scaling,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating MultiSignalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The two datasets are combined into a MultiSignalDataset\n",
    "\n",
    "catalog_idx = 0\n",
    "\n",
    "multi_signal_dataset = MultiSignalDataset(\n",
    "    single_signal_datasets=[catalog_dataset, insitu_dataset],\n",
    "    catalog_idx=catalog_idx,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.arcane2.data.utils import compare_catalogs_for_results, merge_columns_by_mean, shift_columns\n",
    "\n",
    "# We merge the columns and shift them by the time shift\n",
    "df_merged = merge_columns_by_mean(all_results, prefix=\"predicted_value_train_arcane_rtsw_new_bounds_new_drops_\")\n",
    "df_shifted_and_merged = shift_columns(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test the validity of our approach, we generate a catalog from the created ground truth time series\n",
    "\n",
    "original_catalog = catalog_dataset.catalog.event_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nan values\n",
    "df_shifted_and_merged_processed_precat = df_shifted_and_merged.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the percentage of missing values in df_shifted_and_merged_processed_precat\n",
    "expected_range = pd.date_range(\n",
    "    start=df_shifted_and_merged_processed_precat.index.min(),\n",
    "    end=df_shifted_and_merged_processed_precat.index.max(),\n",
    "    freq=\"30min\",\n",
    ")\n",
    "missing = expected_range.difference(df_shifted_and_merged_processed_precat.index)\n",
    "missing_percentage = len(missing) / len(expected_range) * 100\n",
    "print(\n",
    "    f\"Missing values in df_shifted_and_merged_processed_precat: {len(missing)} ({missing_percentage:.2f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectable_original_catalog = []\n",
    "\n",
    "for event in original_catalog:\n",
    "    if event.begin > df_shifted_and_merged_processed_precat.index[0] and event.begin < df_shifted_and_merged_processed_precat.index[-1]:\n",
    "        #calculate the number of datapoints in the time range at a resolution of 30 min\n",
    "        expected_nr_datapoints = int((event.end - event.begin).total_seconds() / 60 / 30)\n",
    "        actual_nr_datapoints = df_shifted_and_merged_processed_precat.loc[event.begin : event.end].shape[0]\n",
    "        if actual_nr_datapoints > expected_nr_datapoints * 0.99:\n",
    "            detectable_original_catalog.append(event)\n",
    "        else:\n",
    "            df_shifted_and_merged_processed_precat.loc[event.begin : event.end] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nan values\n",
    "df_shifted_and_merged_processed = df_shifted_and_merged_processed_precat.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_catalog = EventCatalog(\n",
    "    event_types=\"CME\",\n",
    "    catalog_name=\"True Catalog\",\n",
    "    spacecraft=\"Wind\",\n",
    "    dataframe=df_shifted_and_merged_processed,\n",
    "    key=\"NGUYEN_catalog-ICME\",\n",
    "    resample_freq=\"30min\",\n",
    "    creep_delta=30,\n",
    ").event_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(extracted_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a dataframe with the number of events per month\n",
    "\n",
    "dateindex = pd.date_range(start=\"1998\", end=\"2024\", freq=\"1Y\")\n",
    "\n",
    "event_numbers = pd.DataFrame(\n",
    "    index=dateindex,\n",
    "    columns=[\"Detectable\", \"Extracted\"],\n",
    ")\n",
    "\n",
    "for date in dateindex:\n",
    "    event_numbers.loc[date, \"Detectable\"] = len(\n",
    "        [\n",
    "            x\n",
    "            for x in detectable_original_catalog\n",
    "            if x.begin.year == date.year \n",
    "        ]\n",
    "    )\n",
    "    event_numbers.loc[date, \"Extracted\"] = len(\n",
    "        [\n",
    "            x\n",
    "            for x in extracted_catalog\n",
    "            if x.begin.year == date.year \n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We plot the number of events per month\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "event_numbers.plot(\n",
    "    ax=axs, kind =\"bar\", color=[geo_magenta, geo_lime, geo_cornflowerblue], width=0.8\n",
    ")\n",
    "\n",
    "axs.set_ylabel(\"Number of Events\")\n",
    "axs.set_xlabel(\"Date\")\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5, alpha=0.7)\n",
    "\n",
    "# Format x-axis to show only the year\n",
    "axs.set_xticks(range(0, len(event_numbers.index), 4))  # Show only one tick per year\n",
    "axs.set_xticklabels(event_numbers.index[::4].year, ha=\"center\", rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"Event numbers:\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP, FP, FN, _, found_already, detected, _, ious = compare_catalogs_for_results(\n",
    "    extracted_catalog, detectable_original_catalog\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We expect less events in the extracted catalog than in the original catalog. The results should be perfect scores.\n",
    "\n",
    "print(\"####################################\")\n",
    "print(\" RESULTS FOR GENERATED CATALOG\")\n",
    "print(\"####################################\")\n",
    "print(\"\")\n",
    "print(f\"original: {len(detectable_original_catalog)}\")\n",
    "print(f\"predicted: {len(extracted_catalog)}\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(f\"TP: {len(TP)}\")\n",
    "print(f\"FP: {len(FP)}\")\n",
    "print(f\"FN: {len(FN)}\")\n",
    "\n",
    "predicted = len(extracted_catalog)\n",
    "precision = len(TP) / (len(TP) + len(FP))\n",
    "recall = len(TP) / (len(TP) + len(FN))\n",
    "print(f\"ratio: {predicted/len(TP)}\")\n",
    "print(\"\")\n",
    "print(f\"Precision: {(predicted-len(FP))/(predicted)}\")\n",
    "print(f\"Recall: {len(TP)/len(TP + FN)}\")\n",
    "print(f\"F1: {2*(precision*recall)/(precision+recall)}\")\n",
    "print(\"\")\n",
    "print(f\"mean iou: {np.mean(ious)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the threshold classifier baseline\n",
    "\n",
    "df = multi_signal_dataset.df.copy()\n",
    "\n",
    "from scipy.constants import k, proton_mass, pi\n",
    "import numpy as np\n",
    "\n",
    "v_threshold = 30 * 1e3\n",
    "\n",
    "T_threshold = v_threshold**2 * proton_mass * pi / (8 * k)\n",
    "\n",
    "T_threshold = np.round(T_threshold, -3)\n",
    "\n",
    "b_threshold = 8\n",
    "beta_threshold = 0.3\n",
    "v_threshold = 30\n",
    "\n",
    "print(\n",
    "    f\"Thresholds: T = {T_threshold} K, B = {b_threshold} nT, beta = {beta_threshold}, V = {v_threshold} km/s\"\n",
    ")\n",
    "\n",
    "cols = [\"true_value\", \"predicted_value_threshold\"]\n",
    "\n",
    "result_df = pd.DataFrame(columns=cols, index=df.index)\n",
    "\n",
    "result_df[\"predicted_value_threshold\"] = 0\n",
    "result_df[\"true_value\"] = df[\"NGUYEN_catalog-ICME\"]\n",
    "\n",
    "# Set true_value to 1 only when all three conditions are true\n",
    "result_df.loc[\n",
    "    (df[\"NOAA Realtime Archive_insitu-bt\"] >= b_threshold)\n",
    "    & (df[\"NOAA Realtime Archive_insitu-beta\"] <= beta_threshold)\n",
    "    & (df[\"NOAA Realtime Archive_insitu-tp\"] <= T_threshold),\n",
    "    \"predicted_value_threshold\",\n",
    "] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with the eventwise comparison of the threshold classifier\n",
    "\n",
    "print(\"####################################\")\n",
    "print(\" RESULTS FOR EVENTWISE THRESHOLD CLASSIFIER\")\n",
    "print(\"####################################\")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "threshold_catalog = EventCatalog(\n",
    "    event_types=\"CME\",\n",
    "    catalog_name=\"Threshold Catalog\",\n",
    "    spacecraft=\"OMNI\",\n",
    "    dataframe=result_df,\n",
    "    key=\"predicted_value_threshold\",\n",
    "    creep_delta=30,\n",
    ").event_cat\n",
    "\n",
    "threshold_catalog = [ event for event in threshold_catalog if event.duration > datetime.timedelta(minutes=30)]\n",
    "\n",
    "(\n",
    "    TP_threshold,\n",
    "    FP_threshold,\n",
    "    FN_threshold,\n",
    "    threshold_delays,\n",
    "    found_already_threshold,\n",
    "    detected_threshold,\n",
    "    threshold_durations,\n",
    "    ious_threshold,\n",
    ") = compare_catalogs_for_results(threshold_catalog, extracted_catalog)\n",
    "\n",
    "print(f\"TP: {len(TP_threshold)}\")\n",
    "print(f\"FP: {len(FP_threshold)}\")\n",
    "print(f\"FN: {len(FN_threshold)}\")\n",
    "\n",
    "\n",
    "predicted_threshold = len(threshold_catalog)\n",
    "precision_threshold = len(TP_threshold) / (len(TP_threshold) + len(FP_threshold)) \n",
    "recall_threshold = len(TP_threshold) / (len(TP_threshold) + len(FN_threshold))\n",
    "f1_threshold = (\n",
    "    2\n",
    "    * (precision_threshold * recall_threshold)\n",
    "    / (precision_threshold + recall_threshold)\n",
    ")\n",
    "\n",
    "print(f\"Precision: {precision_threshold}\")\n",
    "print(f\"Recall: {recall_threshold}\")\n",
    "print(f\"F1: {f1_threshold}\")\n",
    "print(f\"mean iou: {np.mean(ious_threshold)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(10, 5))\n",
    "colors = plt.cm.plasma(np.linspace(0, 1, 26))\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.plasma, norm=plt.Normalize(vmin=-26, vmax=-1))\n",
    "\n",
    "resultsdict = {}\n",
    "\n",
    "for t in tqdm(range(1,26, 1)):\n",
    "    key = f\"predicted_value_train_arcane_rtsw_new_bounds_new_drops_tminus{t*2}\"\n",
    "    \n",
    "    thresholds = np.arange(0, 1, 0.1)\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    ious = []\n",
    "    \n",
    "    cat_temp = EventCatalog(\n",
    "            event_types=\"CME\",\n",
    "            spacecraft=\"Wind\",\n",
    "            dataframe=df_shifted_and_merged_processed,\n",
    "            key=key,\n",
    "            creep_delta=30,\n",
    "            thresh=0.1\n",
    "        )\n",
    "    \n",
    "    for event in cat_temp.event_cat:\n",
    "        probability = df_shifted_and_merged_processed.loc[event.begin:event.end, key].mean()\n",
    "        event.proba = probability\n",
    "\n",
    "    for thresh in tqdm(thresholds):\n",
    "        icmes_in_proba = [event for event in cat_temp.event_cat if event.proba > thresh]\n",
    "        TP, FP, FN, _, found_already, detected, _, ious = (\n",
    "        compare_catalogs_for_results(icmes_in_proba, detectable_original_catalog)\n",
    "            )\n",
    "        \n",
    "        predicted = len(icmes_in_proba)\n",
    "\n",
    "        if len(TP) == 0:\n",
    "            ratio = 0\n",
    "        else:\n",
    "            ratio = predicted / len(TP)\n",
    "\n",
    "        if predicted == 0:\n",
    "            precision = 0\n",
    "        else:\n",
    "            precision = len(TP) / (len(TP) + len(FP)) \n",
    "\n",
    "        if len(TP) + len(FN) == 0:\n",
    "            recall = 1\n",
    "        else:\n",
    "            recall = len(TP) / (len(TP) + len(FN))\n",
    "\n",
    "        if precision + recall == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        if precision > 0 and recall > 0:\n",
    "\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            f1s.append(f1)\n",
    "            ious.append(np.mean(ious))\n",
    "\n",
    "    axs.plot(recalls, precisions, color=colors[t], label=\"\")\n",
    "    axs.set_xlim(0.1, 1)\n",
    "    axs.set_ylim(0.1, 1)\n",
    "    axs.set_xlabel(\"Recall\")\n",
    "    axs.set_ylabel(\"Precision\")\n",
    "\n",
    "    max_f1 = np.argmax(f1s)\n",
    "    max_f1_thresh = thresholds[max_f1]\n",
    "    max_f1_recall = recalls[max_f1]\n",
    "    max_f1_precision = precisions[max_f1]\n",
    "    max_f1_f1 = f1s[max_f1]\n",
    "    max_f1_iou = ious[max_f1]\n",
    "\n",
    "    resultsdict[t] = {\n",
    "        \"threshold\": max_f1_thresh,\n",
    "        \"recall\": max_f1_recall,\n",
    "        \"precision\": max_f1_precision,\n",
    "        \"f1\": max_f1_f1,\n",
    "        \"iou\": max_f1_iou,\n",
    "    }\n",
    "\n",
    "cb1 = fig.colorbar(sm, ax=axs, orientation=\"vertical\")\n",
    "\n",
    "ticks = [-1, -5, -10, - 15, -20, -25] \n",
    "tick_labels = [f\"{-t} h\" for t in ticks[::-1]] \n",
    "\n",
    "cb1.set_ticks(ticks)\n",
    "cb1.set_ticklabels(tick_labels)\n",
    "\n",
    "axs.plot([0, recall_threshold, recall_threshold], [precision_threshold, precision_threshold, 0], color=geo_cornflowerblue, label=\"Threshold Classifier\")\n",
    "\n",
    "# show legend\n",
    "axs.legend(loc=\"lower left\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(1, 26, 1)\n",
    "precisions = [resultsdict[i][\"precision\"] for i in t]\n",
    "recalls = [resultsdict[i][\"recall\"] for i in t]\n",
    "f1s = [resultsdict[i][\"f1\"] for i in t]\n",
    "thresholds = [resultsdict[i][\"threshold\"] for i in t]\n",
    "ious = [resultsdict[i][\"iou\"] for i in t]\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "axs.plot(t, f1s, label=\"F1\", color=geo_cornflowerblue)\n",
    "\n",
    "axs.set_xlabel(\"$\\delta$ [hours]\")\n",
    "axs.set_ylabel(\"F1-Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Maximum F1: {max(f1s)} at {t[np.argmax(f1s)]} hours\")\n",
    "print(f\"Precision at maximum F1: {precisions[np.argmax(f1s)]}\")\n",
    "print(f\"Recall at maximum F1: {recalls[np.argmax(f1s)]}\")\n",
    "print(f\"IOU at maximum F1: {ious[np.argmax(f1s)]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning-arcane-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
