{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "This notebook is used to generate the figures in RÃ¼disser et al. 2025.\n",
    "\n",
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().resolve().parent.parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import yaml\n",
    "import pickle\n",
    "from loguru import logger\n",
    "\n",
    "# hide all logs except for errors\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"ERROR\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up matplotlib style\n",
    "sns.set_context(\"talk\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "from scripts.data.visualise.insitu_geosphere import prop\n",
    "\n",
    "geo_cornflowerblue = \"dodgerblue\"\n",
    "geo_lime = \"gold\"\n",
    "geo_magenta = \"firebrick\"\n",
    "\n",
    "from src.arcane2.data.data_utils.event import (\n",
    "    EventCatalog\n",
    ")\n",
    "\n",
    "from src.arcane2.data.utils import compare_catalogs_for_results, merge_columns_by_mean, shift_columns\n",
    "\n",
    "from src.arcane2.data.realtime.realtime_insitu_dataset import RealtimeInsituDataset\n",
    "from src.arcane2.data.catalogs.nguyen_dataset import Nguyen_Dataset\n",
    "from src.arcane2.data.abstract.multi_signal_dataset import MultiSignalDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load the config file used during training\n",
    "\n",
    "config = yaml.safe_load(\n",
    "    open(project_root / \"config/base_dataset/curated_realtime_dataset_lowres.yaml\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "### Loading results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we prepare the filepaths for the results that were generated during inference\n",
    "\n",
    "result_paths = []\n",
    "\n",
    "run_names = [\"train_arcane_rtsw_new_bounds_new_drops\"]\n",
    "for run_name in run_names:\n",
    "    cache_path = project_root / f\"cache/{run_name}\"\n",
    "\n",
    "    path = (\n",
    "        cache_path\n",
    "        / f\"all_results_curated_realtime_dataset_lowres_tminus_all.pkl\"\n",
    "    )\n",
    "    if path.exists():\n",
    "        result_paths.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load the results and concatenate them into a single dataframe\n",
    "\n",
    "for i, path in enumerate(tqdm(result_paths)):\n",
    "    if i == 0:\n",
    "        all_results = pd.read_pickle(path)\n",
    "    else:\n",
    "        loaded = pd.read_pickle(path)\n",
    "        all_results = pd.concat([all_results, loaded], axis=0).sort_index()\n",
    "        all_results = all_results.combine_first(loaded)\n",
    "        all_results = all_results.groupby(all_results.index).first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we drop the missing values\n",
    "\n",
    "all_results = all_results.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the percentage of missing values in all_results\n",
    "expected_range = pd.date_range(\n",
    "    start=all_results.index.min(),\n",
    "    end=all_results.index.max(),\n",
    "    freq=\"30min\",\n",
    ")\n",
    "missing = expected_range.difference(all_results.index)\n",
    "missing_percentage = len(missing) / len(expected_range) * 100\n",
    "print(\n",
    "    f\"Missing values in all_results: {len(missing)} ({missing_percentage:.2f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the model trained correctly, the minimum value should be close to 0\n",
    "print(f\"Minimum value: {all_results[\"predicted_value_train_arcane_rtsw_new_bounds_new_drops_0_tminus1\"].min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Catalog Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the catalog dataset from the Nguyen Catalogs\n",
    "\n",
    "catalog_paths = [\n",
    "    project_root / \"data/dataverse_files/ICME_catalog_OMNI.csv\",\n",
    "    project_root / \"data/dataverse_files/Sheath_catalog_OMNI.csv\",\n",
    "]\n",
    "event_types = config[\"dataset\"][\"single_signal_datasets\"][0].get(\"event_types\", \"ICME\")\n",
    "filters = config[\"dataset\"][\"single_signal_datasets\"][0].get(\"filters\", None)\n",
    "cap = config[\"dataset\"][\"single_signal_datasets\"][0].get(\"cap\", None)\n",
    "resample_freq = config[\"dataset\"][\"single_signal_datasets\"][0].get(\n",
    "    \"resample_freq\", \"10min\"\n",
    ")\n",
    "\n",
    "catalog_dataset = Nguyen_Dataset(\n",
    "    folder_paths=catalog_paths,\n",
    "    resample_freq=resample_freq,\n",
    "    event_types=event_types,\n",
    "    filters=filters,\n",
    "    cap=cap,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating In Situ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the insitu dataset from the NOAA archive, interpolating the data\n",
    "\n",
    "folder_path = project_root / \"data/noaa_archive_gsm.p\"\n",
    "components = config[\"dataset\"][\"single_signal_datasets\"][1].get(\"components\")\n",
    "resample = config[\"dataset\"][\"single_signal_datasets\"][1].get(\"resample\")\n",
    "resample_method = config[\"dataset\"][\"single_signal_datasets\"][1].get(\"resample_method\")\n",
    "resample_freq = config[\"dataset\"][\"single_signal_datasets\"][1].get(\"resample_freq\")\n",
    "padding = config[\"dataset\"][\"single_signal_datasets\"][1].get(\"padding\")\n",
    "lin_interpol = config['dataset']['single_signal_datasets'][1].get('lin_interpol')\n",
    "scaling = config[\"dataset\"][\"single_signal_datasets\"][1].get(\"scaling\", \"None\")\n",
    "\n",
    "insitu_dataset = RealtimeInsituDataset(\n",
    "    folder_path=folder_path,\n",
    "    components=components,\n",
    "    resample=resample,\n",
    "    resample_freq=resample_freq,\n",
    "    resample_method=resample_method,\n",
    "    padding=padding,\n",
    "    lin_interpol=lin_interpol,\n",
    "    scaling=scaling,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating MultiSignalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The two datasets are combined into a MultiSignalDataset\n",
    "\n",
    "catalog_idx = 0\n",
    "\n",
    "multi_signal_dataset = MultiSignalDataset(\n",
    "    single_signal_datasets=[catalog_dataset, insitu_dataset],\n",
    "    catalog_idx=catalog_idx,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We merge the columns and shift them by the time shift\n",
    "df_merged = merge_columns_by_mean(all_results, prefix=\"predicted_value_train_arcane_rtsw_new_bounds_new_drops_\")\n",
    "df_shifted_and_merged = shift_columns(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test the validity of our approach, we generate a catalog from the created ground truth time series\n",
    "\n",
    "original_catalog = catalog_dataset.catalog.event_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nan values\n",
    "df_shifted_and_merged_processed_precat = df_shifted_and_merged.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the percentage of missing values in df_shifted_and_merged_processed_precat\n",
    "expected_range = pd.date_range(\n",
    "    start=df_shifted_and_merged_processed_precat.index.min(),\n",
    "    end=df_shifted_and_merged_processed_precat.index.max(),\n",
    "    freq=\"30min\",\n",
    ")\n",
    "missing = expected_range.difference(df_shifted_and_merged_processed_precat.index)\n",
    "missing_percentage = len(missing) / len(expected_range) * 100\n",
    "print(\n",
    "    f\"Missing values in df_shifted_and_merged_processed_precat: {len(missing)} ({missing_percentage:.2f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectable_original_catalog = []\n",
    "\n",
    "for event in original_catalog:\n",
    "    if event.begin > df_shifted_and_merged_processed_precat.index[0] and event.begin < df_shifted_and_merged_processed_precat.index[-1]:\n",
    "        #calculate the number of datapoints in the time range at a resolution of 30 min\n",
    "        expected_nr_datapoints = int((event.end - event.begin).total_seconds() / 60 / 30)\n",
    "        actual_nr_datapoints = df_shifted_and_merged_processed_precat.loc[event.begin : event.end].shape[0]\n",
    "        if actual_nr_datapoints > expected_nr_datapoints * 0.99:\n",
    "            detectable_original_catalog.append(event)\n",
    "        else:\n",
    "            df_shifted_and_merged_processed_precat.loc[event.begin : event.end] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nan values\n",
    "df_shifted_and_merged_processed = df_shifted_and_merged_processed_precat.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_catalog = EventCatalog(\n",
    "    event_types=\"CME\",\n",
    "    catalog_name=\"True Catalog\",\n",
    "    spacecraft=\"Wind\",\n",
    "    dataframe=df_shifted_and_merged_processed,\n",
    "    key=\"NGUYEN_catalog-ICME\",\n",
    "    resample_freq=\"30min\",\n",
    "    creep_delta=30,\n",
    ").event_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(extracted_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a dataframe with the number of events per month\n",
    "\n",
    "dateindex = pd.date_range(start=\"1998\", end=\"2024\", freq=\"1Y\")\n",
    "\n",
    "event_numbers = pd.DataFrame(\n",
    "    index=dateindex,\n",
    "    columns=[\"Detectable\", \"Extracted\"], \n",
    ")\n",
    "\n",
    "for date in dateindex:\n",
    "    event_numbers.loc[date, \"Detectable\"] = len(\n",
    "        [\n",
    "            x\n",
    "            for x in detectable_original_catalog\n",
    "            if x.begin.year == date.year \n",
    "        ]\n",
    "    )\n",
    "    event_numbers.loc[date, \"Extracted\"] = len(\n",
    "        [\n",
    "            x\n",
    "            for x in extracted_catalog\n",
    "            if x.begin.year == date.year \n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We plot the number of events per month\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "event_numbers.plot(\n",
    "    ax=axs, kind =\"bar\", color=[geo_magenta, geo_lime, geo_cornflowerblue], width=0.8\n",
    ")\n",
    "\n",
    "axs.set_ylabel(\"Number of Events\")\n",
    "axs.set_xlabel(\"Date\")\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5, alpha=0.7)\n",
    "\n",
    "# Format x-axis to show only the year\n",
    "axs.set_xticks(range(0, len(event_numbers.index), 4))\n",
    "axs.set_xticklabels(event_numbers.index[::4].year, ha=\"center\", rotation=0)\n",
    "\n",
    "# add minor ticks\n",
    "axs.xaxis.set_minor_locator(plt.MultipleLocator(1))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"../../plots/event_numbers.pdf\")\n",
    "\n",
    "print(\"Event numbers:\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP, FP, FN, _, found_already, detected, _, ious = compare_catalogs_for_results(\n",
    "    extracted_catalog, detectable_original_catalog\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in FN:\n",
    "    event.plot_mag(df_shifted_and_merged_processed, delta = 32, pred_key=\"true_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We expect less events in the extracted catalog than in the original catalog. The results should be perfect scores.\n",
    "\n",
    "print(\"####################################\")\n",
    "print(\" RESULTS FOR GENERATED CATALOG\")\n",
    "print(\"####################################\")\n",
    "print(\"\")\n",
    "print(f\"original: {len(detectable_original_catalog)}\")\n",
    "print(f\"predicted: {len(extracted_catalog)}\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(f\"TP: {len(TP)}\")\n",
    "print(f\"FP: {len(FP)}\")\n",
    "print(f\"FN: {len(FN)}\")\n",
    "\n",
    "predicted = len(extracted_catalog)\n",
    "precision = len(TP) / (len(TP) + len(FP)) #(predicted - len(FP)) / (predicted)\n",
    "recall = len(TP) / (len(TP) + len(FN))\n",
    "print(f\"ratio: {predicted/len(TP)}\")\n",
    "print(\"\")\n",
    "print(f\"Precision: {(predicted-len(FP))/(predicted)}\")\n",
    "print(f\"Recall: {len(TP)/len(TP + FN)}\")\n",
    "print(f\"F1: {2*(precision*recall)/(precision+recall)}\")\n",
    "print(\"\")\n",
    "print(f\"mean iou: {np.mean(ious)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the threshold classifier baseline\n",
    "\n",
    "df = multi_signal_dataset.df.copy()\n",
    "\n",
    "from scipy.constants import k, proton_mass, pi\n",
    "import numpy as np\n",
    "\n",
    "v_threshold = 30 * 1e3\n",
    "\n",
    "T_threshold = v_threshold**2 * proton_mass * pi / (8 * k)\n",
    "\n",
    "T_threshold = np.round(T_threshold, -3)\n",
    "\n",
    "b_threshold = 8\n",
    "beta_threshold = 0.3\n",
    "v_threshold = 30\n",
    "\n",
    "print(\n",
    "    f\"Thresholds: T = {T_threshold} K, B = {b_threshold} nT, beta = {beta_threshold}, V = {v_threshold} km/s\"\n",
    ")\n",
    "\n",
    "cols = [\"true_value\", \"predicted_value_threshold\"]\n",
    "\n",
    "result_df = pd.DataFrame(columns=cols, index=df.index)\n",
    "\n",
    "result_df[\"predicted_value_threshold\"] = 0\n",
    "result_df[\"true_value\"] = df[\"NGUYEN_catalog-ICME\"]\n",
    "\n",
    "# Set true_value to 1 only when all three conditions are true\n",
    "result_df.loc[\n",
    "    (df[\"NOAA Realtime Archive_insitu-bt\"] >= b_threshold)\n",
    "    & (df[\"NOAA Realtime Archive_insitu-beta\"] <= beta_threshold)\n",
    "    & (df[\"NOAA Realtime Archive_insitu-tp\"] <= T_threshold),\n",
    "    \"predicted_value_threshold\",\n",
    "] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with the eventwise comparison of the threshold classifier\n",
    "\n",
    "print(\"####################################\")\n",
    "print(\" RESULTS FOR EVENTWISE THRESHOLD CLASSIFIER\")\n",
    "print(\"####################################\")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "threshold_catalog = EventCatalog(\n",
    "    event_types=\"CME\",\n",
    "    catalog_name=\"Threshold Catalog\",\n",
    "    spacecraft=\"OMNI\",\n",
    "    dataframe=result_df,\n",
    "    key=\"predicted_value_threshold\",\n",
    "    creep_delta=30,\n",
    ").event_cat\n",
    "\n",
    "threshold_catalog = [ event for event in threshold_catalog if event.duration > datetime.timedelta(minutes=30)]\n",
    "\n",
    "(\n",
    "    TP_threshold,\n",
    "    FP_threshold,\n",
    "    FN_threshold,\n",
    "    threshold_delays,\n",
    "    found_already_threshold,\n",
    "    detected_threshold,\n",
    "    threshold_durations,\n",
    "    ious_threshold,\n",
    ") = compare_catalogs_for_results(threshold_catalog, extracted_catalog)\n",
    "\n",
    "print(f\"TP: {len(TP_threshold)}\")\n",
    "print(f\"FP: {len(FP_threshold)}\")\n",
    "print(f\"FN: {len(FN_threshold)}\")\n",
    "\n",
    "\n",
    "predicted_threshold = len(threshold_catalog)\n",
    "precision_threshold = len(TP_threshold) / (len(TP_threshold) + len(FP_threshold)) \n",
    "recall_threshold = len(TP_threshold) / (len(TP_threshold) + len(FN_threshold))\n",
    "f1_threshold = (\n",
    "    2\n",
    "    * (precision_threshold * recall_threshold)\n",
    "    / (precision_threshold + recall_threshold)\n",
    ")\n",
    "\n",
    "print(f\"Precision: {precision_threshold}\")\n",
    "print(f\"Recall: {recall_threshold}\")\n",
    "print(f\"F1: {f1_threshold}\")\n",
    "print(f\"mean iou: {np.mean(ious_threshold)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to redo the analysis\n",
    "\n",
    "# resultsdict = {}\n",
    "\n",
    "# things_to_plot = {}\n",
    "\n",
    "# for t in tqdm(range(1,51, 1)):\n",
    "#     key = f\"predicted_value_train_arcane_rtsw_new_bounds_new_drops_tminus{t}\"\n",
    "    \n",
    "#     things_to_plot[key] = {}\n",
    "    \n",
    "#     thresholds = np.arange(0.1, 1, 0.1)\n",
    "#     precisions = []\n",
    "#     recalls = []\n",
    "#     f1s = []\n",
    "#     ious_all = []\n",
    "#     delays_all = []\n",
    "#     durations_all = []\n",
    "#     FPs = []\n",
    "#     TPs = []\n",
    "#     FNs = []\n",
    "#     predicteds = []\n",
    "\n",
    "\n",
    "#     for thresh in tqdm(thresholds):\n",
    "#         cat_temp = EventCatalog(\n",
    "#             event_types=\"CME\",\n",
    "#             spacecraft=\"Wind\",\n",
    "#             dataframe=df_shifted_and_merged_processed,\n",
    "#             key=key,\n",
    "#             creep_delta=30,\n",
    "#             thresh=thresh,\n",
    "#         )\n",
    "\n",
    "#         icmes_in_proba = cat_temp.event_cat\n",
    "        \n",
    "#         TP, FP, FN, delays, found_already, detected, durations, ious = (\n",
    "#         compare_catalogs_for_results(icmes_in_proba, extracted_catalog) #detectable_original_catalog)\n",
    "#             )\n",
    "        \n",
    "#         predicted = len(icmes_in_proba)\n",
    "\n",
    "#         if len(TP) == 0:\n",
    "#             ratio = 0\n",
    "#         else:\n",
    "#             ratio = predicted / len(TP)\n",
    "\n",
    "#         if len(TP) + len(FP) == 0: #predicted == 0:\n",
    "#             precision = 0\n",
    "#         else:\n",
    "#             precision = len(TP) / (len(TP) + len(FP)) #(predicted - len(FP)) / predicted\n",
    "\n",
    "#         if len(TP) + len(FN) == 0:\n",
    "#             recall = 1\n",
    "#         else:\n",
    "#             recall = len(TP) / (len(TP) + len(FN))\n",
    "\n",
    "#         if precision + recall == 0:\n",
    "#             f1 = 0\n",
    "#         else:\n",
    "#             f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "#         if precision > 0 and recall > 0:\n",
    "\n",
    "#             precisions.append(precision)\n",
    "#             recalls.append(recall)\n",
    "#             f1s.append(f1)\n",
    "#             ious_all.append(ious)\n",
    "#             delays_all.append(delays)\n",
    "#             durations_all.append(durations)\n",
    "#             FPs.append(FP)\n",
    "#             TPs.append(TP)\n",
    "#             FNs.append(FN)\n",
    "#             predicteds.append(icmes_in_proba)\n",
    "    \n",
    "#     things_to_plot[key]['precisions'] = precisions\n",
    "#     things_to_plot[key]['recalls'] = recalls\n",
    "\n",
    "\n",
    "#     max_f1 = np.argmax(f1s)\n",
    "#     max_f1_thresh = thresholds[max_f1]\n",
    "#     max_f1_recall = recalls[max_f1]\n",
    "#     max_f1_precision = precisions[max_f1]\n",
    "#     max_f1_f1 = f1s[max_f1]\n",
    "#     max_f1_iou = ious_all[max_f1]\n",
    "#     max_f1_delay = delays_all[max_f1]\n",
    "#     max_f1_duration = durations_all[max_f1]\n",
    "#     max_f1_FP = FPs[max_f1]\n",
    "#     max_f1_TP = TPs[max_f1]\n",
    "#     max_f1_FN = FNs[max_f1]\n",
    "#     max_f1_predicted = predicteds[max_f1]\n",
    "\n",
    "#     resultsdict[key] = {\n",
    "#         \"threshold\": max_f1_thresh,\n",
    "#         \"recall\": max_f1_recall,\n",
    "#         \"precision\": max_f1_precision,\n",
    "#         \"f1\": max_f1_f1,\n",
    "#         \"iou\": max_f1_iou,\n",
    "#         \"delay\": max_f1_delay,\n",
    "#         \"duration\": max_f1_duration,\n",
    "#         \"FP\": max_f1_FP,\n",
    "#         \"TP\": max_f1_TP,\n",
    "#         \"FN\": max_f1_FN,\n",
    "#         \"predicted\": max_f1_predicted,\n",
    "#     }\n",
    "\n",
    "# # save result_dict to file\n",
    "\n",
    "# with open(\"resultsdict_strict_extracted.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(resultsdict, f)\n",
    "\n",
    "# # save things_to_plot to file\n",
    "\n",
    "# with open(\"things_to_plot_extracted.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(things_to_plot, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"things_to_plot_extracted.pkl\", \"rb\") as f:\n",
    "    things_to_plot = pickle.load(f)\n",
    "\n",
    "with open(\"resultsdict_strict_extracted.pkl\", \"rb\") as f:\n",
    "    resultsdict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "colors = plt.cm.plasma(np.linspace(0, 1, 51))\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.plasma, norm=plt.Normalize(vmin=-51, vmax=-1))\n",
    "\n",
    "\n",
    "for i, key in enumerate(things_to_plot.keys()):\n",
    "\n",
    "    recalls = things_to_plot[key][\"recalls\"] \n",
    "    precisions = things_to_plot[key][\"precisions\"] \n",
    "\n",
    "    axs.plot(recalls, precisions, color=colors[i], label=\"\")\n",
    "    axs.set_xlim(0.1, 1)\n",
    "    axs.set_ylim(0.1, 1)\n",
    "    axs.set_xlabel(\"Recall\")\n",
    "    axs.set_ylabel(\"Precision\")\n",
    "\n",
    "    # plot dashed horiuontal lines\n",
    "    recalls_contin = [recalls[-1]] + [0]\n",
    "    precis_contin = [precisions[-1]]+[precisions[-1]]\n",
    "\n",
    "    axs.plot(recalls_contin, precis_contin, color=colors[i], linestyle = \"dotted\") #, linewidth = 0.2)\n",
    "\n",
    "cb1 = fig.colorbar(sm, ax=axs, orientation=\"vertical\")\n",
    "\n",
    "ticks = [-2, -10, -20, - 30, -40, -50]\n",
    "tick_labels = [f\"{-int(t/2)} h\" for t in ticks[::-1]] \n",
    "\n",
    "cb1.set_ticks(ticks)\n",
    "cb1.set_ticklabels(tick_labels)\n",
    "\n",
    "axs.plot([0, recall_threshold, recall_threshold], [precision_threshold, precision_threshold, 0], color=geo_cornflowerblue, label=\"Threshold Classifier\")\n",
    "\n",
    "# show legend\n",
    "axs.legend(loc=\"lower left\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"../../plots/precision_recall_curves.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(1,51, 1)\n",
    "\n",
    "resultsdictkeys = list(resultsdict.keys())\n",
    "\n",
    "precisions = [resultsdict[resultsdictkeys[i-1]][\"precision\"] for i in t]\n",
    "recalls = [resultsdict[resultsdictkeys[i-1]][\"recall\"] for i in t]\n",
    "f1s = [resultsdict[resultsdictkeys[i-1]][\"f1\"] for i in t]\n",
    "thresholds = [resultsdict[resultsdictkeys[i-1]][\"threshold\"] for i in t]\n",
    "ious = [resultsdict[resultsdictkeys[i-1]][\"iou\"] for i in t]\n",
    "durations_all = [resultsdict[resultsdictkeys[i-1]][\"duration\"] for i in t]\n",
    "delays_all = [resultsdict[resultsdictkeys[i-1]][\"delay\"] for i in t]\n",
    "FPs = [resultsdict[resultsdictkeys[i-1]][\"FP\"] for i in t]\n",
    "TPs = [resultsdict[resultsdictkeys[i-1]][\"TP\"] for i in t]\n",
    "FNs = [resultsdict[resultsdictkeys[i-1]][\"FN\"] for i in t]\n",
    "predicteds = [resultsdict[resultsdictkeys[i-1]][\"predicted\"] for i in t]\n",
    "\n",
    "mean_rel_delays = []\n",
    "mean_delays = []\n",
    "mean_rel_start_time_errors = []\n",
    "mean_start_time_errors = []\n",
    "\n",
    "for wt in t:\n",
    "    corrected_delays = [or_delay + wt * 60 /2 if or_delay > 0 else wt * 60 /2 for or_delay in resultsdict[resultsdictkeys[wt-1]][\"delay\"]]\n",
    "    start_time_error = [or_delay if or_delay > 0 else -or_delay for or_delay in resultsdict[resultsdictkeys[wt-1]][\"delay\"]]\n",
    "\n",
    "    durations = resultsdict[resultsdictkeys[wt-1]][\"duration\"]\n",
    "    rel_delays = np.array(corrected_delays) / np.array(durations)\n",
    "    rel_start_time_error = np.array(start_time_error) / np.array(durations)\n",
    "    \n",
    "    mean_rel_delays.append(np.mean(rel_delays) * 100)\n",
    "    mean_rel_start_time_errors.append(np.mean(rel_start_time_error) * 100)\n",
    "    mean_delays.append(np.mean(corrected_delays)/60)\n",
    "    mean_start_time_errors.append(np.mean(start_time_error)/60)\n",
    "\n",
    "corrected_threshold_delays = [or_delay + 0 * 60 if or_delay > 0 else 0 * 60 for or_delay in threshold_delays]\n",
    "start_time_error_threshold = [or_delay if or_delay > 0 else -or_delay for or_delay in threshold_delays]\n",
    "rel_delays_threshold = np.array(corrected_threshold_delays) / np.array(threshold_durations) * 100\n",
    "rel_start_time_error_threshold = np.array(start_time_error_threshold) / np.array(threshold_durations) * 100\n",
    "mean_rel_delays_threshold = np.mean(rel_delays_threshold)\n",
    "mean_rel_start_time_errors_threshold = np.mean(rel_start_time_error_threshold)\n",
    "mean_delays_threshold = np.mean(corrected_threshold_delays)/60\n",
    "mean_start_time_error_threshold = np.mean(start_time_error_threshold)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "axs.plot(t/2, f1s, label=\"F1-Score\", color=geo_cornflowerblue)\n",
    "\n",
    "axs.set_xlabel(\"$\\delta$ [hours]\")\n",
    "axs.set_ylabel(\"F1-Score\")\n",
    "\n",
    "# plot mean_rel_delays on a secondary y-axis\n",
    "axs2 = axs.twinx()\n",
    "axs2.plot(t/2, mean_rel_delays, label=\"Mean Relative Delay\", color=geo_lime)\n",
    "\n",
    "axs2.set_ylabel(\"Percentage of Duration [%]\")\n",
    "\n",
    "axs.legend(loc=\"upper left\")\n",
    "axs2.legend(loc=\"lower right\")\n",
    "fig.savefig(\"../../plots/pr_rec_waiting_eventwise.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "combined_fig, combined_axs = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "colors = plt.cm.plasma(np.linspace(0, 1, 51))\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.plasma, norm=plt.Normalize(vmin=-51, vmax=-1))\n",
    "\n",
    "for i, key in enumerate(things_to_plot.keys()):\n",
    "\n",
    "    recalls_to_plot = things_to_plot[key][\"recalls\"] # + [0]\n",
    "    precisions_to_plot = things_to_plot[key][\"precisions\"] #+ [things_to_plot[key][\"precisions\"][-1]] \n",
    "\n",
    "    combined_axs[0].plot(recalls_to_plot, precisions_to_plot, color=colors[i], label=\"\")\n",
    "    combined_axs[0].set_xlim(0, 1)\n",
    "    combined_axs[0].set_ylim(0, 1)\n",
    "    combined_axs[0].set_xlabel(\"Recall\")\n",
    "    combined_axs[0].set_ylabel(\"Precision\")\n",
    "\n",
    "    # plot dashed horiuontal lines\n",
    "    recalls_contin = [recalls_to_plot[-1]] + [0]\n",
    "    precis_contin = [precisions_to_plot[-1]]+[precisions_to_plot[-1]]\n",
    "\n",
    "    combined_axs[0].plot(recalls_contin, precis_contin, color=colors[i], linestyle = \"dotted\") #, linewidth = 0.2)\n",
    "\n",
    "# Create an axis divider for the upper subplot\n",
    "divider = make_axes_locatable(combined_axs[0])\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.15)\n",
    "\n",
    "cb1 = combined_fig.colorbar(sm, cax=cax, orientation=\"vertical\")\n",
    "\n",
    "# add label to colorbar below\n",
    "cb1.set_label(\"$\\delta$ [hours]\")\n",
    "\n",
    "ticks = [-2, -10, -20, - 30, -40, -50] \n",
    "tick_labels = [f\"{-int(t/2)}\" for t in ticks[::-1]] \n",
    "\n",
    "cb1.set_ticks(ticks)\n",
    "cb1.set_ticklabels(tick_labels)\n",
    "\n",
    "combined_axs[0].plot([0, recall_threshold, recall_threshold], [precision_threshold, precision_threshold, 0], color=geo_cornflowerblue, label=\"Threshold Classifier\")\n",
    "\n",
    "# show legend\n",
    "combined_axs[0].legend(loc=\"lower left\")\n",
    "\n",
    "combined_axs[1].plot(t/2, f1s, label=\"F1-Score\", color=geo_cornflowerblue)\n",
    "\n",
    "combined_axs[1].set_xlabel(\"$\\delta$ [hours]\")\n",
    "combined_axs[1].set_ylabel(\"F1-Score\")\n",
    "\n",
    "combined_axs[1].set_xlim(0, 25)\n",
    "\n",
    "# plot mean_rel_delays on a secondary y-axis\n",
    "axs2 = combined_axs[1].twinx()\n",
    "axs2.plot(t/2, mean_rel_delays, label=\"Mean Relative Delay\", color=geo_lime)\n",
    "axs2.set_ylabel(\"Mean Relative Delay [%]\")\n",
    "\n",
    "combined_axs[1].legend(loc=\"upper left\")\n",
    "axs2.legend(loc=\"lower right\")\n",
    "\n",
    "combined_axs[0].grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5, alpha=0.7)\n",
    "combined_axs[1].grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5, alpha=0.7)\n",
    "axs2.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5, alpha=0.7)\n",
    "\n",
    "# Add subplot labels\n",
    "combined_axs[0].text(-0.15, 1.05, \"a)\", transform=combined_axs[0].transAxes,\n",
    "                     fontweight='bold', va='top', ha='left')\n",
    "\n",
    "combined_axs[1].text(-0.15, 1.05, \"b)\", transform=combined_axs[1].transAxes,\n",
    "                     fontweight='bold', va='top', ha='left')\n",
    "\n",
    "axs2.set_ylim(0, 100)\n",
    "\n",
    "combined_fig.tight_layout()\n",
    "\n",
    "plt.savefig(\"../../plots/combined_results.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Maximum F1: {max(f1s)} at {t[np.argmax(f1s)]/2} hours\")\n",
    "print(f\"Precision at maximum F1: {precisions[np.argmax(f1s)]}\")\n",
    "print(f\"Recall at maximum F1: {recalls[np.argmax(f1s)]}\")\n",
    "print(f\"IOU at maximum F1: {np.mean(ious[np.argmax(f1s)])}\")\n",
    "print(f\"Mean Delay at maximum F1: {mean_delays[np.argmax(f1s)]}\")\n",
    "print(f\"Mean Relative Delay at maximum F1: {mean_rel_delays[np.argmax(f1s)]}\")\n",
    "print(f\"Mean Start Time Error at maximum F1: {mean_start_time_errors[np.argmax(f1s)]}\")\n",
    "print(f\"Mean Relative Start Time Error at maximum F1: {mean_rel_start_time_errors[np.argmax(f1s)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Minimum Delay: {min(mean_delays)} at {t[np.argmin(mean_delays)]/2} hours\")\n",
    "print(f\"Precision at minimum Delay: {precisions[np.argmin(mean_delays)]}\")\n",
    "print(f\"Recall at minimum Delay: {recalls[np.argmin(mean_delays)]}\")\n",
    "print(f\"IOU at minimum Delay: {np.mean(ious[np.argmin(mean_delays)])}\")\n",
    "print(f\"Mean Relative Delay at minimum Delay: {mean_rel_delays[np.argmin(mean_delays)]}\")\n",
    "print(f\"F1 at minimum Delay: {f1s[np.argmin(mean_delays)]}\")\n",
    "print(f\"Mean Start Time Error at minimum Delay: {mean_start_time_errors[np.argmin(mean_delays)]}\")\n",
    "print(f\"Mean Relative Start Time Error at minimum Delay: {mean_rel_start_time_errors[np.argmin(mean_delays)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_ts = [\n",
    "    0, # 30 min\n",
    "    5, # 3 hours\n",
    "    11, # 6 hours\n",
    "    23, # 12 hours\n",
    "]\n",
    "\n",
    "selected_durations = [\n",
    "    0.5,\n",
    "    3,\n",
    "    6,\n",
    "    12,\n",
    "]\n",
    "\n",
    "hist_selected = {}\n",
    "delays_mean_selected = {}\n",
    "delays_rel_mean_selected = {}\n",
    "\n",
    "# Define bins\n",
    "bins = np.linspace(0, 100, 21)  # 20 bins from 0 to 100\n",
    "\n",
    "wt_colors =[\"black\", geo_magenta, geo_lime, geo_cornflowerblue]\n",
    "\n",
    "for i, wt in enumerate(selected_ts):\n",
    "    fp_wt = FPs[wt]\n",
    "    tp_wt = TPs[wt]\n",
    "    fn_wt = FNs[wt]\n",
    "    predicted_wt = predicteds[wt]\n",
    "    delays_wt =[or_delay + selected_durations[i] * 60 if or_delay > 0 else  selected_durations[i]  * 60 for or_delay in delays_all[wt]]\n",
    "    start_time_error_wt = [or_delay if or_delay > 0 else -or_delay for or_delay in delays_all[wt]]\n",
    "    durations_wt = durations_all[wt]\n",
    "    rel_delays_wt = np.array(delays_wt) / np.array(durations_wt) * 100\n",
    "    rel_start_time_error_wt = np.array(start_time_error_wt) / np.array(durations_wt) * 100\n",
    "\n",
    "    precision_wt = precisions[wt]\n",
    "    recall_wt = recalls[wt]\n",
    "    f1_wt = f1s[wt]\n",
    "    iou_wt = ious[wt]\n",
    "\n",
    "    print(f\"####################################\")\n",
    "    print(f\" RESULTS FOR WAITING TIME {selected_durations[i]} HOURS\")\n",
    "    print(f\"####################################\")\n",
    "    print(\"\")\n",
    "    print(f\"TP: {len(tp_wt)}\")\n",
    "    print(f\"FP: {len(fp_wt)}\")\n",
    "    print(f\"FN: {len(fn_wt)}\")\n",
    "    print(\"\")\n",
    "    print(f\"Precision: {precision_wt}\")\n",
    "    print(f\"Recall: {recall_wt}\")\n",
    "    print(f\"F1: {f1_wt}\")\n",
    "    print(f\"IOU: {np.mean(iou_wt)}\")\n",
    "    print(f\"Mean Delay: {np.mean(delays_wt)/60}\")\n",
    "    print(f\"Mean Relative Delay: {np.mean(rel_delays_wt)}\")\n",
    "    print(f\"Mean Start Time Error: {np.mean(start_time_error_wt)/60}\")\n",
    "    print(f\"Mean Relative Start Time Error: {np.mean(rel_start_time_error_wt)}\")\n",
    "\n",
    "    hist, bin_edges = np.histogram(rel_delays_wt, bins=bins)\n",
    "\n",
    "    hist_selected[str( selected_durations[i] )] = hist\n",
    "    delays_mean_selected[str( selected_durations[i] )] = np.mean(delays_wt)\n",
    "    delays_rel_mean_selected[str( selected_durations[i] )] = np.mean(rel_delays_wt)\n",
    "\n",
    "print(f\"####################################\")\n",
    "print(f\" RESULTS FOR THRESHOLD CLASSIFIER\")\n",
    "print(f\"####################################\")\n",
    "print(\"\")\n",
    "print(f\"TP: {len(TP_threshold)}\")\n",
    "print(f\"FP: {len(FP_threshold)}\")\n",
    "print(f\"FN: {len(FN_threshold)}\")\n",
    "print(\"\")\n",
    "print(f\"Precision: {precision_threshold}\")\n",
    "print(f\"Recall: {recall_threshold}\")\n",
    "print(f\"F1: {f1_threshold}\")\n",
    "print(f\"IOU: {np.mean(ious_threshold)}\")\n",
    "print(f\"Mean Delay: {np.mean(corrected_threshold_delays)/60}\")\n",
    "print(f\"Mean Relative Delay: {np.mean(rel_delays_threshold)}\")\n",
    "print(f\"Mean Start Time Error: {np.mean(start_time_error_threshold)/60}\")\n",
    "print(f\"Mean Relative Start Time Error: {np.mean(rel_start_time_error_threshold)}\")\n",
    "\n",
    "hist_threshold, _ = np.histogram(rel_delays_threshold, bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X positions for the CDF\n",
    "x_positions = bin_edges[1:]\n",
    "\n",
    "# Create the figure\n",
    "fig, axs = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "wt_colors =[\"black\", geo_magenta, geo_lime, geo_cornflowerblue]\n",
    "\n",
    "for i, wt in enumerate(selected_ts):\n",
    "\n",
    "\n",
    "    cdf = np.cumsum(hist_selected[str(selected_durations[i])]) / len(detectable_original_catalog) * 100\n",
    "    \n",
    "    axs.plot(x_positions, cdf, label=f\"ARCANE Classifier, $\\delta = {selected_durations[i]}$ h\", color=wt_colors[i], marker=\"o\", linestyle=\"-\")\n",
    "\n",
    "cdf_threshold = np.cumsum(hist_threshold) / len(detectable_original_catalog) * 100\n",
    "\n",
    "axs.plot(x_positions, cdf_threshold, label=\"Threshold Classifier\", color=\"k\", marker=\"s\", linestyle=\"--\")\n",
    "\n",
    "axs.set_xticks(np.arange(0, 101, 20))\n",
    "\n",
    "axs.set_xlabel(\"Delay [%]\")\n",
    "axs.set_ylabel(\"Cumulative Number of Events [%]\")\n",
    "axs.legend(loc=\"lower right\")\n",
    "axs.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.7)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"../../plots/delay_cdf.pdf\", bbox_inches=\"tight\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We calculate the mean absolute and relative delays\n",
    "print(\"\")\n",
    "print(\"####################################\")\n",
    "print(\" RESULTS FOR DELAYS\")\n",
    "print(\"####################################\")\n",
    "\n",
    "for i, wt in enumerate(selected_ts):\n",
    "    print(f\"Results for waiting time {selected_durations[i]} hours\")\n",
    "    print(f\"mean absolute delay: {delays_mean_selected[str( selected_durations[i] )]/60}\")\n",
    "    print(f\"mean relative delay: {delays_rel_mean_selected[str( selected_durations[i] )]}\")\n",
    "    \n",
    "    print(\"\")\n",
    "\n",
    "print(f\"mean absolute delay thresholds: {mean_delays_threshold}\")\n",
    "print(f\"mean delay thresholds: {mean_rel_delays_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_t = 0\n",
    "\n",
    "wt = selected_ts[selected_t]\n",
    "\n",
    "bmaxes_wt_tp = []\n",
    "bmaxes_wt_fp = []\n",
    "bmaxes_wt_fn = []\n",
    "\n",
    "betameans_wt_tp = []\n",
    "betameans_wt_fp = []\n",
    "betameans_wt_fn = []\n",
    "\n",
    "vmaxes_wt_tp = []\n",
    "vmaxes_wt_fp = []\n",
    "vmaxes_wt_fn = []\n",
    "\n",
    "delays_wt =np.array([or_delay + selected_durations[selected_t] * 60 if or_delay > 0 else  selected_durations[selected_t]  * 60 for or_delay in delays_all[wt]]) / 60\n",
    "durations_wt = np.array(durations_all[wt]) / 60\n",
    "rel_delays_wt = np.array(delays_wt) / np.array(durations_wt) * 100\n",
    "max_thresh_wt = thresholds[wt]\n",
    "\n",
    "cat_wt = EventCatalog(\n",
    "    event_types=\"CME\",\n",
    "    spacecraft=\"Wind\",\n",
    "    dataframe=df_shifted_and_merged_processed,\n",
    "    key=resultsdictkeys[wt],\n",
    "    creep_delta=30,\n",
    "    thresh=max_thresh_wt,\n",
    ")\n",
    "\n",
    "TPs_wt, FPs_wt, FNs_wt, delays_wt, found_already_wt, detected_wt, durations_wt, ious_wt = compare_catalogs_for_results(cat_wt.event_cat, extracted_catalog) \n",
    "\n",
    "for i, event in enumerate(TPs_wt):\n",
    "    bmax = event.get_value(df_shifted_and_merged_processed, \"NOAA Realtime Archive_insitu-bt\", \"max\")\n",
    "    bmaxes_wt_tp.append(bmax)\n",
    "    betamean = event.get_value(df_shifted_and_merged_processed, \"NOAA Realtime Archive_insitu-beta\", \"mean\")\n",
    "    betameans_wt_tp.append(betamean)\n",
    "    vmax = event.get_value(df_shifted_and_merged_processed, \"NOAA Realtime Archive_insitu-vt\", \"max\")\n",
    "    vmaxes_wt_tp.append(vmax)\n",
    "\n",
    "for i, event in enumerate(FPs_wt):\n",
    "    bmax = event.get_value(df_shifted_and_merged_processed, \"NOAA Realtime Archive_insitu-bt\", \"max\")\n",
    "    bmaxes_wt_fp.append(bmax)\n",
    "    betamean = event.get_value(df_shifted_and_merged_processed, \"NOAA Realtime Archive_insitu-beta\", \"mean\")\n",
    "    betameans_wt_fp.append(betamean)\n",
    "    vmax = event.get_value(df_shifted_and_merged_processed, \"NOAA Realtime Archive_insitu-vt\", \"max\")\n",
    "    vmaxes_wt_fp.append(vmax)\n",
    "\n",
    "for i, event in enumerate(FNs_wt):\n",
    "    bmax = event.get_value(df_shifted_and_merged_processed, \"NOAA Realtime Archive_insitu-bt\", \"max\")\n",
    "    bmaxes_wt_fn.append(bmax)\n",
    "    betamean = event.get_value(df_shifted_and_merged_processed, \"NOAA Realtime Archive_insitu-beta\", \"mean\")\n",
    "    betameans_wt_fn.append(betamean)\n",
    "    vmax = event.get_value(df_shifted_and_merged_processed, \"NOAA Realtime Archive_insitu-vt\", \"max\")\n",
    "    vmaxes_wt_fn.append(vmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "markersize = 5\n",
    "\n",
    "alpha = 1\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "gs = GridSpec(3, 3, height_ratios=[1, 3, 0.001], width_ratios=[3, 1, 0.001])\n",
    "\n",
    "ax_scatter = fig.add_subplot(gs[1, 0])  # Main scatter plot\n",
    "\n",
    "\n",
    "first_tp, first_fp, first_fn = True, True, True\n",
    "\n",
    "for i, bmax in enumerate(bmaxes_wt_tp):\n",
    "    label = \"TP\" if first_tp else \"\"\n",
    "    ax_scatter.plot(bmax, betameans_wt_tp[i], \"o\", color=geo_lime, label=label, markersize = markersize)\n",
    "    first_tp = False\n",
    "\n",
    "for i, bmax in enumerate(bmaxes_wt_fp):\n",
    "    label = \"FP\" if first_fp else \"\"\n",
    "    ax_scatter.plot(bmax, betameans_wt_fp[i], \"o\", color=geo_cornflowerblue, label=label, markersize = markersize)\n",
    "    first_fp = False\n",
    "    \n",
    "for i, bmax in enumerate(bmaxes_wt_fn):\n",
    "    label = \"FN\" if first_fn else \"\"\n",
    "    ax_scatter.plot(bmax, betameans_wt_fn[i], \"o\", color=geo_magenta, label=label, markersize = markersize)\n",
    "    first_fn = False\n",
    "\n",
    "ax_scatter.legend(loc=\"upper right\")\n",
    "\n",
    "\n",
    "ax_scatter.set_xlabel(\"B$_{max}$ [nT]\")\n",
    "ax_scatter.set_ylabel(\"$\\\\beta_{mean}$ [km/s]\")\n",
    "ax_scatter.tick_params(axis='both', which='both')\n",
    "ax_scatter.set_yscale(\"log\")\n",
    "\n",
    "ax_kde = fig.add_subplot(gs[0, 0], sharex=ax_scatter)\n",
    "sns.kdeplot(bmaxes_wt_tp, ax=ax_kde, color=geo_lime, fill=False, alpha=alpha)\n",
    "sns.kdeplot(bmaxes_wt_fp, ax=ax_kde, color=geo_cornflowerblue, fill=False, alpha=alpha)\n",
    "sns.kdeplot(bmaxes_wt_fn, ax=ax_kde, color=geo_magenta, fill=False, alpha=alpha)\n",
    "\n",
    "ax_kde.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)  # Hide x-axis ticks and labels\n",
    "ax_kde.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)   # Hide y-axis ticks and labels\n",
    "ax_kde.set_ylabel(\"\")\n",
    "ax_kde.spines[\"bottom\"].set_visible(False)\n",
    "ax_kde.spines[\"top\"].set_visible(False)\n",
    "ax_kde.spines[\"right\"].set_visible(False)\n",
    "ax_kde.spines[\"left\"].set_visible(False)\n",
    "\n",
    "\n",
    "# Right KDE plot (sharing y-axis with scatter plot)\n",
    "ax_kde2 = fig.add_subplot(gs[1, 1], sharey=ax_scatter)\n",
    "\n",
    "sns.kdeplot(betameans_wt_tp, ax=ax_kde2, color=geo_lime, fill=False, alpha=alpha, vertical=True)\n",
    "sns.kdeplot(betameans_wt_fp, ax=ax_kde2, color=geo_cornflowerblue, fill=False, alpha=alpha, vertical=True)\n",
    "sns.kdeplot(betameans_wt_fn, ax=ax_kde2, color=geo_magenta, fill=False, alpha=alpha, vertical=True)\n",
    "\n",
    "ax_kde2.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False, labelleft= False)  # Hide x-axis ticks and labels\n",
    "ax_kde2.tick_params(axis='y', which='both', left=False, right=False, labelleft=False, labelbottom = False)   # Hide y-axis ticks and labels\n",
    "ax_kde2.set_xlabel(\"\")\n",
    "ax_kde2.spines[\"left\"].set_visible(False)\n",
    "ax_kde2.spines[\"top\"].set_visible(False)\n",
    "ax_kde2.spines[\"right\"].set_visible(False)\n",
    "ax_kde2.spines[\"bottom\"].set_visible(False)\n",
    "\n",
    "# Adjust layout to minimize overlap and reduce spacing\n",
    "fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "\n",
    "# Save figures\n",
    "fig.savefig(\"../../plots/bmax_betamean_kde.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "gs = GridSpec(3, 3, height_ratios=[1, 3, 0.001], width_ratios=[3, 1, 0.001])\n",
    "\n",
    "ax_scatter = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "first_tp, first_fp, first_fn = True, True, True\n",
    "\n",
    "for i, bmax in enumerate(bmaxes_wt_tp):\n",
    "    label = \"TP\" if first_tp else \"\"\n",
    "    ax_scatter.plot(bmax, vmaxes_wt_tp[i], \"o\", color=geo_lime, label=label, markersize = markersize)\n",
    "    first_tp = False\n",
    "\n",
    "for i, bmax in enumerate(bmaxes_wt_fp):\n",
    "    label = \"FP\" if first_fp else \"\"\n",
    "    ax_scatter.plot(bmax, vmaxes_wt_fp[i], \"o\", color=geo_cornflowerblue, label=label, markersize = markersize)\n",
    "    first_fp = False\n",
    "\n",
    "for i, bmax in enumerate(bmaxes_wt_fn):\n",
    "    label = \"FN\" if first_fn else \"\"\n",
    "    ax_scatter.plot(bmax, vmaxes_wt_fn[i], \"o\", color=geo_magenta, label=label, markersize = markersize)\n",
    "    first_fn = False\n",
    "\n",
    "ax_scatter.legend(loc=\"upper right\")\n",
    "\n",
    "\n",
    "ax_scatter.set_xlabel(\"B$_{max}$ [nT]\")\n",
    "ax_scatter.set_ylabel(\"V$_{max}$ [km/s]\")\n",
    "ax_scatter.tick_params(axis='both', which='both')\n",
    "\n",
    "ax_kde = fig.add_subplot(gs[0, 0], sharex=ax_scatter)\n",
    "sns.kdeplot(bmaxes_wt_tp, ax=ax_kde, color=geo_lime, fill=False, alpha=alpha)\n",
    "sns.kdeplot(bmaxes_wt_fp, ax=ax_kde, color=geo_cornflowerblue, fill=False, alpha=alpha)\n",
    "sns.kdeplot(bmaxes_wt_fn, ax=ax_kde, color=geo_magenta, fill=False, alpha=alpha)\n",
    "\n",
    "ax_kde.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)  # Hide x-axis ticks and labels\n",
    "ax_kde.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)   # Hide y-axis ticks and labels\n",
    "ax_kde.set_ylabel(\"\")\n",
    "ax_kde.spines[\"bottom\"].set_visible(False)\n",
    "ax_kde.spines[\"top\"].set_visible(False)\n",
    "ax_kde.spines[\"right\"].set_visible(False)\n",
    "ax_kde.spines[\"left\"].set_visible(False)\n",
    "\n",
    "# Right KDE plot (sharing y-axis with scatter plot)\n",
    "ax_kde2 = fig.add_subplot(gs[1, 1], sharey=ax_scatter)\n",
    "\n",
    "sns.kdeplot(vmaxes_wt_tp, ax=ax_kde2, color=geo_lime, fill=False, alpha=alpha, vertical=True)\n",
    "sns.kdeplot(vmaxes_wt_fp, ax=ax_kde2, color=geo_cornflowerblue, fill=False, alpha=alpha, vertical=True)\n",
    "sns.kdeplot(vmaxes_wt_fn, ax=ax_kde2, color=geo_magenta, fill=False, alpha=alpha, vertical=True)\n",
    "\n",
    "ax_kde2.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False, labelleft= False)  # Hide x-axis ticks and labels\n",
    "ax_kde2.tick_params(axis='y', which='both', left=False, right=False, labelleft=False, labelbottom = False)   # Hide y-axis ticks and labels\n",
    "ax_kde2.set_xlabel(\"\")\n",
    "ax_kde2.spines[\"left\"].set_visible(False)\n",
    "ax_kde2.spines[\"top\"].set_visible(False)\n",
    "ax_kde2.spines[\"right\"].set_visible(False)\n",
    "ax_kde2.spines[\"bottom\"].set_visible(False)\n",
    "ax_scatter.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.7)\n",
    "ax_scatter.set_xlim(0,100)\n",
    "\n",
    "# Adjust layout to minimize overlap and reduce spacing\n",
    "fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "\n",
    "# Save figures\n",
    "fig.savefig(\"../../plots/bmax_vmax_kde.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of detected events: {len(detected_wt)}\")\n",
    "\n",
    "event = detected_wt[62]\n",
    "\n",
    "data = event.get_data(df_shifted_and_merged_processed, delta=12)\n",
    "\n",
    "bt_keys = [col for col in data.columns if \"bt\" in col]\n",
    "bx_keys = [col for col in data.columns if \"bx\" in col]\n",
    "by_keys = [col for col in data.columns if \"by\" in col]\n",
    "bz_keys = [col for col in data.columns if \"bz\" in col]\n",
    "\n",
    "fig, axes = plt.subplots(1,1,figsize=(12,4), sharex=True)\n",
    "axes.plot(data.index, data[bt_keys[0]], label = \"|B|\", color = \"black\")\n",
    "axes.plot(data.index, data[bx_keys[0]], label = \"B$_x$\", color = geo_magenta)\n",
    "axes.plot(data.index, data[by_keys[0]], label = \"B$_y$\", color = geo_lime)\n",
    "axes.plot(data.index, data[bz_keys[0]], label = \"B$_z$\", color = geo_cornflowerblue)\n",
    "\n",
    "for true_event in extracted_catalog:\n",
    "    if true_event.begin > event.begin - datetime.timedelta(hours=6) and true_event.end < event.end + datetime.timedelta(hours=6):\n",
    "        axes.axvline(true_event.begin, color=\"black\", linestyle=\"--\")\n",
    "        axes.axvline(true_event.end, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "for pred_event in predicted_wt:\n",
    "    if pred_event.begin > event.begin - datetime.timedelta(hours=6) and pred_event.end < event.end + datetime.timedelta(hours=6):\n",
    "        #color the background between predicted event begin and end\n",
    "        axes.axvspan(pred_event.begin, pred_event.end, color=geo_lime, alpha=0.2)\n",
    "\n",
    "axes.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter(\"%Y-%m-%d\"))\n",
    "axes.xaxis.set_major_locator(plt.matplotlib.dates.DayLocator(interval=1))\n",
    "\n",
    "# set the yaxis label\n",
    "axes.set_ylabel(\"Magnetic Field [nT]\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig(\"../../plots/event62.pdf\")\n",
    "\n",
    "event = detected_wt[456] \n",
    "already = False\n",
    "data = event.get_data(df_shifted_and_merged_processed, delta=36)\n",
    "bt_keys = [col for col in data.columns if \"bt\" in col]\n",
    "bx_keys = [col for col in data.columns if \"bx\" in col]\n",
    "by_keys = [col for col in data.columns if \"by\" in col]\n",
    "bz_keys = [col for col in data.columns if \"bz\" in col]\n",
    "\n",
    "fig, axes = plt.subplots(1,1,figsize=(12,4), sharex=True)\n",
    "axes.plot(data.index, data[bt_keys[0]], label = \"|B|\", color = \"black\")\n",
    "axes.plot(data.index, data[bx_keys[0]], label = \"B$_x$\", color = geo_magenta)\n",
    "axes.plot(data.index, data[by_keys[0]], label = \"B$_y$\", color = geo_lime)\n",
    "axes.plot(data.index, data[bz_keys[0]], label = \"B$_z$\", color = geo_cornflowerblue)\n",
    "\n",
    "for true_event in extracted_catalog:\n",
    "    if true_event.begin > event.begin - datetime.timedelta(hours=72) and true_event.end < event.end + datetime.timedelta(hours=72):\n",
    "        if already== False:\n",
    "            axes.axvline(true_event.begin, color=\"black\", linestyle=\"--\")\n",
    "            axes.axvline(true_event.end, color=\"black\", linestyle=\"--\")\n",
    "            already = True\n",
    "        else:\n",
    "            axes.axvline(true_event.begin, color=\"red\", linestyle=\"--\")\n",
    "            axes.axvline(true_event.end, color=\"red\", linestyle=\"--\")\n",
    "            already = False\n",
    "\n",
    "for pred_event in predicted_wt:\n",
    "    if pred_event.begin > event.begin - datetime.timedelta(hours=72) and pred_event.end < event.end + datetime.timedelta(hours=72):\n",
    "        #color the background between predicted event begin and end\n",
    "        axes.axvspan(pred_event.begin, pred_event.end, color=geo_lime, alpha=0.2)\n",
    "axes.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter(\"%Y-%m-%d\"))\n",
    "axes.xaxis.set_major_locator(plt.matplotlib.dates.DayLocator(interval=1))\n",
    "\n",
    "# set the yaxis label\n",
    "axes.set_ylabel(\"Magnetic Field [nT]\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"../../plots/event456.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "events_to_plot  = [3, 62, 92, 456]\n",
    "\n",
    "deltas = [12, 6, 12, 24]\n",
    "\n",
    "delta2s = [12, 6, 12, 8]\n",
    "labels = [\"a)\", \"b)\", \"c)\", \"d)\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10), sharex=False)\n",
    "axes = axes.flatten() \n",
    "\n",
    "for i, (event_idx, delta) in enumerate(zip(events_to_plot, deltas)):\n",
    "    event = detected_wt[event_idx]\n",
    "    data = event.get_data(df_shifted_and_merged_processed, delta=delta, delta2=delta2s[i])\n",
    "\n",
    "    bt_keys = [col for col in data.columns if \"bt\" in col]\n",
    "    bx_keys = [col for col in data.columns if \"bx\" in col]\n",
    "    by_keys = [col for col in data.columns if \"by\" in col]\n",
    "    bz_keys = [col for col in data.columns if \"bz\" in col]\n",
    "\n",
    "    ax = axes[i]\n",
    "    ax.plot(data.index, data[bt_keys[0]], label=\"|B|\", color=\"black\")\n",
    "    ax.plot(data.index, data[bx_keys[0]], label=\"B$_x$\", color=geo_magenta)\n",
    "    ax.plot(data.index, data[by_keys[0]], label=\"B$_y$\", color=geo_lime)\n",
    "    ax.plot(data.index, data[bz_keys[0]], label=\"B$_z$\", color=geo_cornflowerblue)\n",
    "\n",
    "    # Set x-axis limits to the data range to eliminate extra whitespace\n",
    "    ax.set_xlim(data.index.min(), data.index.max())\n",
    "\n",
    "    ax.text(-0.15, 1.05, labels[i], transform=ax.transAxes,\n",
    "                     fontweight='bold', va='top', ha='left')\n",
    "    ax.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.7)\n",
    "\n",
    "    line_count = 0  # Counter for vertical lines\n",
    "    for true_event in extracted_catalog:\n",
    "        if true_event.begin > event.begin - datetime.timedelta(hours=delta*3) and true_event.end < event.end + datetime.timedelta(hours=delta*3):\n",
    "            color = \"black\" if line_count < 2 else \"red\"\n",
    "            ax.axvline(true_event.begin, color=color, linestyle=\"--\")\n",
    "            line_count += 1\n",
    "            color = \"black\" if line_count < 2 else \"red\"\n",
    "            ax.axvline(true_event.end, color=color, linestyle=\"--\")\n",
    "            line_count += 1\n",
    "\n",
    "    for pred_event in predicted_wt:\n",
    "        if pred_event.begin > event.begin - datetime.timedelta(hours=delta*3) and pred_event.end < event.end + datetime.timedelta(hours=delta):\n",
    "            ax.axvspan(pred_event.begin, pred_event.end, color=\"grey\", alpha=0.2, zorder=0)\n",
    "\n",
    "    ax.set_ylabel(\"B [nT]\")\n",
    "\n",
    "    dates_year = data.index.min().year\n",
    "\n",
    "    ax.set_xlabel(str(dates_year))\n",
    "    \n",
    "    locator = mdates.DayLocator(interval=1)\n",
    "    ax.xaxis.set_major_locator(locator)\n",
    "    \n",
    "    # Set x-axis formatter to 'Nov 08' style\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%b %d\"))\n",
    "\n",
    "# Add a global legend for the entire figure\n",
    "lines_labels = [ax.get_legend_handles_labels() for ax in axes]\n",
    "lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]\n",
    "fig.legend(lines[:4], labels[:4], loc='upper center', ncol=4, frameon=True, bbox_to_anchor=(0.5, 1.05))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"../../plots/examples.pdf\", bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning-arcane-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
